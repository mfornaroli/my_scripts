{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classifier\n",
    "# find the best hyperplane that separates classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(698, 9)\n",
      "(698,)\n",
      "0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, cross_validation, neighbors, svm\n",
    "col_list = ['id', 'clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion', 'single_epith_cell_size', 'bare_nuclei', 'bland_chrom', 'norm_nucl', 'mitosis', 'Class' ]\n",
    "df = pd.read_csv(r'C:\\Users\\Mfornaroli\\Downloads\\breast-cancer-wisconsin.data')\n",
    "df.columns=col_list\n",
    "df.replace('?', -99999, inplace=True) \n",
    "df.drop('id', axis=1, inplace=True)\n",
    "X = np.array(df.drop(['Class'], 1))\n",
    "y = np.array(df.Class)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size = 0.2)\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectors: direction and module. \n",
    "dot product (component by component, and sum)\n",
    "creating decision boundaries\n",
    "boundary: described by a vector? (perpendicular to it i guess?)\n",
    "unknown point: described by another vector\n",
    "Eq describing decision boundary: {dot product between vectors + bias = 0}\n",
    "for one class the equation comes +1, for the other -1 (not clear)\n",
    "need to find: decision function. now it gets...weird\n",
    "we want to locate support vectors, apparently. equations to derive them. \n",
    "support vectors (+ and -1) --> make the equation be... \n",
    "x_+sv dot w + b = +1   for class +\n",
    "x_-sv dot w + b = -1   for class -\n",
    "width of the hyperplanes that 'go through' these support vectors? (distance between them...)*\n",
    "support vectors: points that the two hyperplanes with the most distance\n",
    "between them pass through\n",
    "support vector definition: feature set that determine the hyperplane... \n",
    "if we move it, it affects best separation. \n",
    "width/2 will give us our plane!*\n",
    "we want to maximize that width, to get the best separating hyperplane. \n",
    "width = to be maximized = (x_+sv - x_-sv)*(w/module(w)) \n",
    "= (...) = 2/module(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so: our classification will be done:\n",
    "(weight vector) dot product (feature vector)\n",
    "best 'line' defined by weight vector w. \n",
    "distance from separating line: how confident we are in our prediction. \n",
    "we want to  have the largest 'margin'.\n",
    "ok so i ve verified: we can define a plane as:\n",
    "(vector X dot normal_vect + bias = 0 )\n",
    "given a point A (vecotor), algebra --> distance between point A and plane turn out to be:\n",
    "norm_vect dot vect_A + bias (same bias!)\n",
    "check: https://www.youtube.com/watch?v=ax8LxRZCORU\n",
    "and https://www.youtube.com/watch?v=YuyeOErjrOM\n",
    "and also: https://www.youtube.com/watch?v=_PwhiWxHK8o&t=2s\n",
    "so now that we know how to express the distance, lets find the best plane, with the highest margin (distance...)\n",
    "prediction = sign(norm_vect dot vect_A + bias)\n",
    "confidence = norm of distance\n",
    "we want to find w (and b?) so that the smalles margin (distance module) is as large as possible (kind of)\n",
    "support vectors: dots that are determined to be nearest to the best dec boundary (given a definition of optimization that includes the possibility of outliers, that are allowed within a certain budget)\n",
    "so: best line can be found... looking somehow at the support vectors only. \n",
    "JUST CHECK THE INTERNET FOR EXHAUSTIVE EXPLANATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solving the optimization problem\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: array([[1, 7],\n",
      "       [2, 8],\n",
      "       [3, 8]]), 1: array([[ 5,  1],\n",
      "       [ 6, -1],\n",
      "       [ 7,  3]])}\n"
     ]
    }
   ],
   "source": [
    "data_dict = {-1: np.array([[1,7],\n",
    "                          [2,8],\n",
    "                          [3,8]]), \n",
    "             +1: np.array([[5,1],\n",
    "                          [6,-1],\n",
    "                          [7,3]])}\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building our SVM class\n",
    "class Supp_Vect_Machine:\n",
    "    def __init__(self, visualization=True):\n",
    "        self.visualization = visualization\n",
    "        self.colors = {1: 'r', -1: 'b'}\n",
    "        if self.visualization:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(1,1,1) # check out matplotlib tutorial man\n",
    "   \n",
    "    #training step, dude. optimization: there are much better ways  \n",
    "    def fit(self, data):\n",
    "        self.data = data # now the algo can reference data\n",
    "        opt_dict = {}\n",
    "        transforms = [[1,1],  # basically, each time we have a vector, we want to transform it       \n",
    "                     [-1,1],  \n",
    "                     [-1,-1],\n",
    "                     [1,-1]]\n",
    "        all_data = []\n",
    "        for yi in self.data:\n",
    "            for featureset in self.data[yi]:\n",
    "                for feature in featureset:\n",
    "                    all_data.append(feature)\n",
    "        # searching for max and min values\n",
    "        self.max_feature_value = max(all_data)\n",
    "        self.min_feature_value = min(all_data)\n",
    "        all_data = None\n",
    "        # first big steps, then smaller as we approach the minimum\n",
    "        step_sizes = [self.max_feature_value * 0.1,   # big steps\n",
    "                     self.max_feature_value * 0.01,\n",
    "                      # point of expense:\n",
    "                     self.max_feature_value * 0.001]\n",
    "        # extremely expensive\n",
    "        b_range_multiple = 5   # b doesnt need to be as precise as w\n",
    "        b_multiple = 5\n",
    "        latest_optimum = self.max_feature_value * 10  # first elemnt \n",
    "        \n",
    "        # begin actual stepping process\n",
    "        # we dont need to take as small steps with b as we do with w\n",
    "        for step in spet_sizes:\n",
    "            w = np.array([latest_optimum, latest_optimum])\n",
    "            # we can do it because its convex, we know its gonna have a solution.. \n",
    "            optimized = False # will stay false until we have no more steps down to take\n",
    "            while not optimized:\n",
    "                # we want to have minimum module of w and maximum bias b\n",
    "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple), \n",
    "                                   self.max_feature_value*b_range_multiple,\n",
    "                                  step*b_multiple):\n",
    "                    for transformation in transforms:\n",
    "                        # we are going to run through a bunch of w that have components between -80 and +80 in our case\n",
    "                        w_t = w*transformation\n",
    "                        found_option = True\n",
    "                        # weakest link in SVM basically\n",
    "                        # SMO attempts to fix it.... \n",
    "                        # we know have to run the calculation for all of the data!\n",
    "                        # yi(w . xi + b) >= 1   this is our constraint\n",
    "                        for i in self.data:  #i is the class\n",
    "                            for xi in self.data[i]:  \n",
    "                                yi = i\n",
    "                                if not yi*(np.dot(w_t, xi)+b) >= 1:\n",
    "                                    found_option = False\n",
    "                                    # and maybe put a break here?\n",
    "                                    \n",
    "                        if found_option: # everything checked out\n",
    "                            opt_dict[np.linalg.norm(w_t)] = [w_t, b]\n",
    "                    if w[0] < 0:\n",
    "                        optimized = True\n",
    "                        print(\"optimized a step.\")\n",
    "                    else:\n",
    "                        w = w - step\n",
    "                norms = sorted([n for n in opt_dict])\n",
    "                \n",
    "                opt_choice = opt_dict[norms[0]]\n",
    "                self.w = opt_choice[0]\n",
    "                self.b = opt_choice[1]\n",
    "                latest_optimum = opt_choice[0][0] + step*2\n",
    "    \n",
    "    def predict(self, feature):\n",
    "        # sign(x dot w + b)\n",
    "        classification = np.sign(np.dot(np.array(features), self.w) + self.b)\n",
    "        if classification != 0 and self.visualization:\n",
    "            self.ax.scatter(features[0], features[1], marker='*', c=self.colors[classification])\n",
    "        return classification\n",
    "    \n",
    "    def visualize(self):\n",
    "        [[self.ax.scatter(x[0], x[1], s=100, colors=self.colors[i]) for x in data_dict[i]] for i in data_dict]\n",
    "        \n",
    "    def hyperplane(x, w, b, v):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's really getting too long and complicated... to be completed maybe another day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
